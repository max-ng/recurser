
<a name="readme-top"></a>




<!-- ABOUT THE PROJECT -->
# Recurser

A new model and implementation to reduce VRAM usage on transformer models.



## Online demos
Reduce the VRAM usage of GPT2-XL by 25%, so that we can run GPT2-XL with Pytorch in a colab or with our gpu.

[[Colab]](https://colab.research.google.com/drive/1UHlRvaSnAZz6T8H_Nj_BhDqMtSBCMlYT?usp=sharing)


## Installation
Always install the library from PyPI:

  ```sh
    pip install recursers
  ```



## Todos

- [ ] Re-implement recurser for other models
- [ ] Enable MPS acceleration on Mac
- [ ] Retraining: The model training of the recurser is a little different from the usual. 

<p align="right">(<a href="#readme-top">back to top</a>)</p>


## Reference
Karpathy's elegant GPT implementation\
https://github.com/karpathy/nanoGPT

Hugging Face's library\
https://github.com/huggingface/transformers













